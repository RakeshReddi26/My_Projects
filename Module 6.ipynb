{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    - UnsupervisedML_Netflix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Unsupervised ML\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** - Rakesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "The primary goal of this project is to conduct an exploratory analysis and clustering analysis on a dataset of Netflix movies and TV shows, aiming to gain insights into content distribution, trends over the years, and clustering of similar content based on text features.\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "For the given Dataset we have to\n",
        "\n",
        "Imported Libraries\n",
        "Loaded Dataset\n",
        "EDA and Data Vizualization\n",
        "Data Cleaning and Feature Engineering\n",
        "Making some Hypothesis From Data Visulaized\n",
        "Pick Appropriate Model and train\n",
        "Prediction and some evaluate matrices for model\n",
        "Conculsion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/RakeshReddi26/My_Projects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Netflix, a leading streaming service, has experienced significant changes in its content landscape over the years, with a notable increase in TV shows and a decrease in the number of movies. The challenge is to explore and analyze a dataset containing information on Netflix movies and TV shows as of 2019, sourced from Flixable. The primary goal is to derive meaningful insights into content distribution across countries, discern any shifts in focus from movies to TV shows, and cluster similar content based on text features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "netflix_data = pd.read_csv(\"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look (To get 1st look of the data set (1st five rows as (.head), last five rows as (.tail)))\n",
        "netflix_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "netflix_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "netflix_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "netflix_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "netflix_data.isna().sum().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "netflix_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYavsNcImRDL"
      },
      "outputs": [],
      "source": [
        "# To show the percentage of null value for particular column\n",
        "nullvalue_percentage = netflix_data.isna().sum()/len(netflix_data) * 100\n",
        "nullvalue_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyLrUUFjmTYg"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=nullvalue_percentage.index, y=nullvalue_percentage)\n",
        "plt.title('Percentage of Null Values in Each Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Percentage Null')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "The dataset contains information about movies and TV shows available on Netflix as of 2019. It was collected from Flixable, a third-party Netflix search engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "netflix_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "netflix_data.describe()    # Result is only release year column which is because of only one column in numeric data type in our data set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "The primary features in the dataset include:\n",
        "\n",
        "show_id: A unique identifier for each show.\n",
        "\n",
        "type: Indicates whether the entry is a movie or a TV show.\n",
        "\n",
        "title: The title of the movie or TV show.\n",
        "\n",
        "director: The director(s) of the movie or TV show.\n",
        "\n",
        "cast: The cast or actors in the movie or TV show.\n",
        "\n",
        "country: The country or countries where the content is available.\n",
        "\n",
        "date_added: The date when the content was added to Netflix.\n",
        "\n",
        "release_year: The year the movie or TV show was released.\n",
        "\n",
        "rating: The content rating.\n",
        "\n",
        "duration: The duration of the movie or TV show.\n",
        "\n",
        "listed_in: Categories or genres the content is listed under.\n",
        "\n",
        "description: A brief description of the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "netflix_data.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# It describes the attribute with object datatype\n",
        "netflix_data.describe(include=[\"object\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZurAt0imsgw"
      },
      "outputs": [],
      "source": [
        "#To show the data types\n",
        "netflix_data.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10yDVFdmuZs"
      },
      "outputs": [],
      "source": [
        "# Impute missing values in other columns (e.g., director, cast, country)\n",
        "netflix_data['country'].fillna('Unknown', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irFA-So_mv9H"
      },
      "outputs": [],
      "source": [
        "netflix_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq4nLCTRmy82"
      },
      "outputs": [],
      "source": [
        "# Convert 'date_added' to datetime format\n",
        "netflix_data['date_added'] = pd.to_datetime(netflix_data['date_added'], errors='coerce')\n",
        "\n",
        "# Check for any values that couldn't be converted\n",
        "print(netflix_data['date_added'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjyateFBnEH4"
      },
      "outputs": [],
      "source": [
        "# Extract month and year from 'date_added' for further analysis\n",
        "netflix_data['added_day'] = netflix_data['date_added'].dt.day\n",
        "netflix_data['added_month'] = netflix_data['date_added'].dt.month\n",
        "netflix_data['added_year'] = netflix_data['date_added'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUyaTsG9nFyS"
      },
      "outputs": [],
      "source": [
        "#To show the data types\n",
        "netflix_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VXHdwiNnIwu"
      },
      "outputs": [],
      "source": [
        "#To show columns\n",
        "netflix_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGAaa8qnnKwO"
      },
      "outputs": [],
      "source": [
        "netflix_data['director'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaT95l5dnNHZ"
      },
      "outputs": [],
      "source": [
        "netflix_data['cast'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhbH3USknPTG"
      },
      "outputs": [],
      "source": [
        "netflix_data['country'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "Initial Data Overview:\n",
        "\n",
        "Checked the first and last few rows of the dataset to get a glimpse of its structure. Checked the number of rows and columns in the dataset (7787 rows, 12 columns). Examined basic information about the dataset using info() and describe().\n",
        "\n",
        "Handling Duplicate Values:\n",
        "\n",
        "Checked for and identified any duplicate rows in the dataset (no duplicates found).\n",
        "\n",
        "Handling Missing Values:\n",
        "\n",
        "Explored and visualized missing values in different columns. Addressed missing values in columns like 'director', 'cast', 'country', 'date_added', and 'rating'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "netflix_data.boxplot(column =['release_year'], grid = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "To show outliers box plot was best way to show those outliers visually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "They are more outliers in release_year column. So we have to remove those outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "If we have more outliers in our dataset then we have to drop those outliers. or else while predicting the model we will not get generalized result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(netflix_data['release_year'], bins=30, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Release Years')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "It is suitable for exploring the distribution of release years in the Netflix dataset, providing a visual representation of the data's central tendency and spread over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "Long-Tail Distribution:\n",
        "\n",
        "The distribution appears to have a long tail, suggesting that there are movies and TV shows from a wide range of release years. This indicates that the Netflix library includes content from both recent years and earlier decades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "The impact of the insights gained from the release year distribution chart on business would depend on the specific goals and strategies of the Netflix platform. Here are potential ways in which these insights could contribute to a positive business impact :\n",
        "\n",
        "User Engagement: If certain release years are associated with a higher number of popular movies or TV shows, Netflix could leverage this information to promote and recommend content from those years. This could enhance user engagement by offering content that aligns with user preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "# Countplot for 'type' column\n",
        "sns.countplot(x='type', data=netflix_data)\n",
        "plt.title('Count of Movies and TV Shows')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "The specific chart chosen, a countplot for the 'type' column, is suitable for visualizing the distribution of categorical data.\n",
        "\n",
        "Comparison: The chart allows for a quick visual comparison between the counts of movies and TV shows. The contrasting bars make it immediately apparent which type is more prevalent in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "The chart illustrates the distribution of entries between movies and TV shows in the Netflix dataset.\n",
        "\n",
        "Prevalence: It is evident that there is a higher count of movies compared to TV shows. This suggests that Netflix has a more extensive collection of movies than TV shows in the dataset.\n",
        "\n",
        "Imbalance: The countplot highlights the imbalance in the number of movies and TV shows, with movies dominating the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Positive Impact:\n",
        "\n",
        "Content Planning: The insights can inform content planning decisions. If Netflix observes a higher demand or viewership for movies, it might decide to focus on acquiring or producing more high-quality movies to cater to user preferences.\n",
        "\n",
        "User Engagement: Understanding the distribution of content types allows Netflix to optimize its user engagement strategies. For example, it can tailor recommendations, personalized playlists, and promotional efforts to align with the predominant content type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "top_countries = netflix_data['country'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette='muted')\n",
        "plt.title('Top 10 Countries with Most Content')\n",
        "plt.xlabel('Number of Titles')\n",
        "plt.ylabel('Country')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "I chose a horizontal bar chart (countplot) to visualize the distribution of content across the top 10 countries because it effectively conveys the relative number of titles for each country. A horizontal bar chart is suitable for displaying the count or frequency of categories, making it easy to compare and rank the top contributors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "United States Dominance: The chart clearly shows that the United States has the highest number of titles on Netflix among the top 10 countries. This dominance is evident by the significantly longer bar for the United States compared to other countries.\n",
        "\n",
        "Diversity in Content: While the United States is a major contributor, there is still diversity in the top 10 countries. Other countries, such as India, the United Kingdom, Canada, and Spain, also have a substantial number of titles, indicating a global presence of content on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Positive Business Impacts:\n",
        "\n",
        "Targeted Marketing: Understanding the countries with the most content allows Netflix to tailor its marketing strategies more effectively. They can run targeted campaigns and promotions in regions with a substantial user base, potentially attracting new subscribers and retaining existing ones.\n",
        "\n",
        "Localization Strategies: Insights into top countries present opportunities for localization strategies. Netflix can invest in creating or acquiring content that resonates with the preferences and cultural nuances of specific regions, enhancing user engagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='rating', data=netflix_data, palette='pastel')\n",
        "plt.title('Distribution of Content Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "chose a countplot to visualize the distribution of content ratings because it provides a clear and concise representation of the frequency of each rating category. Countplots are effective for categorical data, making them suitable for analyzing the distribution of content ratings in this case. The use of colors (pastel palette) aids in differentiating between rating categories, and the rotation of x-axis labels ensures better readability, especially for longer rating names. The goal is to provide a quick and straightforward overview of how content is distributed across different rating categories on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "The countplot for the distribution of content ratings provides insights into the prevalence of different rating categories on Netflix. From the chart:\n",
        "\n",
        "TV-MA (Mature Audience) has the highest count, indicating that a significant portion of the content on Netflix is intended for mature audiences. TV-14 (Parents Strongly Cautioned) is the second most common rating. TV-PG (Parental Guidance Suggested) and R (Restricted) ratings also have a notable presence. Other ratings, such as TV-Y (All Children) and TV-G (General Audience), are less common but still contribute to the overall diversity of content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Yes, the gained insights from the distribution of content ratings can have a positive business impact in several ways:\n",
        "\n",
        "Content Curation: Understanding the distribution of content ratings helps Netflix curate and recommend content more effectively. It allows the platform to provide personalized recommendations to users based on their preferred content ratings.\n",
        "\n",
        "Targeted Marketing: Netflix can use this information for targeted marketing strategies. For example, if mature content (TV-MA) is predominant, marketing efforts can be tailored to reach the mature audience demographic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "# Pie chart for 'rating' distribution\n",
        "rating_counts = netflix_data['rating'].value_counts()\n",
        "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "I chose a pie chart for the distribution of ratings to provide a visual representation of the proportion of each rating category in the overall dataset. The pie chart is suitable for displaying the distribution of categorical data in a way that highlights the relative sizes of each category. This visualization allows for a quick and easy comparison of the prevalence of different ratings on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "Majority of Content is Rated for Mature Audiences:\n",
        "\n",
        "The \"TV-MA\" (Mature Audience) category is the largest slice in the pie chart, indicating that a significant portion of content on Netflix is rated for mature audiences. This suggests that a substantial amount of content may contain explicit or mature content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Positive Business Impacts:\n",
        "\n",
        "Audience Segmentation and Targeting:\n",
        "\n",
        "Understanding the distribution of ratings allows Netflix to effectively segment its audience based on content preferences. This segmentation can inform targeted content recommendations, enhancing user experience and engagement.\n",
        "\n",
        "Negative Growth Considerations:\n",
        "\n",
        "Potential Limited Audience for Certain Ratings:\n",
        "\n",
        "While the diversity of rating categories is a strength, Netflix should be mindful that content rated for specific audiences may have a more limited viewership. Overemphasis on mature content may exclude younger audiences, impacting potential growth in that demographic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "# Pie chart for content types\n",
        "netflix_data['type'].value_counts().plot.pie(autopct='%1.1f%%', explode=[0, 0.05])\n",
        "plt.title('Proportion of Content Types')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "A pie chart is effective for displaying proportions of a whole. In this case, it provides a clear visual representation of the distribution between TV shows and movies on Netflix. The slices of the pie represent the relative sizes of each content type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "ontent Type Distribution:\n",
        "\n",
        "The pie chart clearly shows the distribution of content types on Netflix, indicating what percentage of the library is dedicated to TV shows and movies. Dominance of Movies:\n",
        "\n",
        "The exploded slice for movies suggests that movies have a slightly larger share in the overall content library compared to TV shows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "This visual representation helps Netflix stakeholders and decision-makers quickly grasp the balance between TV shows and movies. It supports strategic planning for content acquisition, production, and user engagement, contributing to a positive business impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code\n",
        "# Count of content types\n",
        "content_type_counts = netflix_data['type'].value_counts()\n",
        "# Donut chart\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.pie(content_type_counts, labels=content_type_counts.index, wedgeprops=dict(width=0.2))\n",
        "plt.title('Donut Chart of Content Types')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "Focus on Count of Content Types:\n",
        "\n",
        "A donut chart is chosen to focus specifically on representing the count of content types (TV shows and movies). The simplicity of the chart allows for a straightforward depiction of the numerical distribution. Visual Appeal:\n",
        "\n",
        "Donut charts are visually appealing and provide an alternative to traditional pie charts. The center of the donut can be utilized for additional information or aesthetics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "Content Type Distribution:\n",
        "\n",
        "The donut chart illustrates the distribution of content types, emphasizing the count of TV shows and movies. Equal Representation:\n",
        "\n",
        "The chart shows that both TV shows and movies contribute significantly to the content library. The width of the donut segments represents their proportional share."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "This donut chart provides a quick numerical overview of the count of TV shows and movies. It can aid decision-makers in understanding the overall composition of the content library, supporting strategic decisions related to content acquisition, user engagement, and platform marketing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code\n",
        "# Count of releases per year\n",
        "release_counts = netflix_data['release_year'].value_counts().sort_index()\n",
        "# Line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x=release_counts.index, y=release_counts.values, marker='o')\n",
        "plt.title('Number of Releases Over the Years')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Releases')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "Reasons for Choosing a Line Chart:\n",
        "\n",
        "Temporal Trend Analysis:\n",
        "\n",
        "A line chart is chosen to analyze the temporal trend in the number of releases over the years. It is suitable for visualizing data points sequentially. Yearly Comparison:\n",
        "\n",
        "The line chart allows for a clear comparison of the number of releases for each year. Viewers can easily identify trends, spikes, or drops over time. Continuous Data Representation:\n",
        "\n",
        "Line charts are effective for representing continuous data, such as the distribution of releases across multiple years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "Release Trend:\n",
        "\n",
        "The line chart shows the trend in the number of releases over the years. It helps identify whether the content library has been growing, stabilizing, or experiencing fluctuations.\n",
        "\n",
        "Peak Years:\n",
        "\n",
        "Peaks in the line indicate years with a higher number of releases. These peak years can be further investigated to understand factors contributing to increased content production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "The line chart provides a historical perspective on the growth or changes in the number of releases. This insight can guide content acquisition strategies, content planning, and resource allocation over time, contributing to informed decision-making for business success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code\n",
        "# Chart - 10 visualization code\n",
        "release_month_counts = netflix_data['added_month'].value_counts().sort_index() # Use the correct column name 'added_month'\n",
        "# Line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x=release_month_counts.index, y=release_month_counts.values, marker='o')\n",
        "plt.title('Number of Releases Over the Years')\n",
        "plt.xlabel('Release Month')\n",
        "plt.ylabel('Number of Releases')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Reasons for Choosing a Line Chart:\n",
        "\n",
        "Temporal Trend Analysis:\n",
        "\n",
        "Similar to the previous line chart, this chart uses a line plot to analyze the temporal trend, but specifically focusing on the distribution of releases across different months.\n",
        "\n",
        "Monthly Comparison:\n",
        "\n",
        "The line chart allows for a clear comparison of the number of releases for each month. Viewers can easily identify patterns or variations in content additions over the months.\n",
        "\n",
        "Continuous Data Representation:\n",
        "\n",
        "Line charts are effective for representing continuous data, such as the distribution of releases across multiple months."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "Monthly Release Patterns:\n",
        "\n",
        "The line chart shows the distribution of releases across months, helping identify patterns or trends in content additions. For example, are certain months associated with higher or lower content releases?\n",
        "\n",
        "Seasonal Variations:\n",
        "\n",
        "Peaks or troughs in specific months may indicate seasonal variations in content additions. This information is valuable for planning content releases based on seasonal preferences or trends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "The chart can guide content release strategies, allowing the platform to align releases with user preferences, seasonal trends, or other factors influencing viewing behavior. This strategic alignment can enhance user engagement and satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code\n",
        "# Count of content types per year\n",
        "content_type_counts = netflix_data.groupby(['release_year', 'type']).size().unstack(fill_value=0)\n",
        "# Stacked bar chart\n",
        "content_type_counts.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='viridis')\n",
        "plt.title('Distribution of Content Types Over the Years')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Releases')\n",
        "plt.legend(title='Content Type')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "Comparison of Content Types:\n",
        "\n",
        "A stacked bar chart is effective for comparing the distribution of content types (Movies and TV Shows) over different release years.\n",
        "\n",
        "Temporal Analysis:\n",
        "\n",
        "The chart provides a temporal analysis of content types, allowing viewers to see how the proportion of Movies and TV Shows has changed over the years.\n",
        "\n",
        "Year-wise Breakdown:\n",
        "\n",
        "Each bar represents a release year, and the segments within the bar (stacks) correspond to the count of Movies and TV Shows for that year. This breakdown aids in understanding the composition of content each year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "Shifts in Content Composition:\n",
        "\n",
        "Changes in the height and composition of the bars indicate shifts in the proportion of Movies and TV Shows released each year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Understanding the historical distribution of content types can inform future content acquisition and production strategies. If certain types of content are more popular in specific years, the platform can tailor its content offerings to align with user preferences during those periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code\n",
        "top_genre = netflix_data['listed_in'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_genre.values, y=top_genre.index)\n",
        "plt.title('Top 10 Most Genre')\n",
        "plt.xlabel('Number of Genre')\n",
        "plt.ylabel('Genre')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "op Genre Comparison:\n",
        "\n",
        "A horizontal bar chart is chosen to compare the top 10 genres based on the number of occurrences in the dataset.\n",
        "\n",
        "Visualizing Genre Distribution:\n",
        "\n",
        "The chart provides a clear visualization of the most popular genres, making it easy to compare their frequencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "Dominant Genres:\n",
        "\n",
        "The chart highlights the genres that are most prevalent in the dataset. Viewers can quickly identify the genres with the highest frequency.\n",
        "\n",
        "Identification of Popular Genres:\n",
        "\n",
        "Users can see which genres are most popular on the platform based on the number of titles in each genre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Knowledge of the most popular genres can influence content acquisition and production decisions. The platform can use this information to invest in genres that have a high viewership, potentially attracting and retaining more subscribers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code\n",
        "most_filmed_director = netflix_data['director'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=most_filmed_director.values, y=most_filmed_director.index)\n",
        "plt.title('Top 10 Most Filmed Director')\n",
        "plt.xlabel('Number of Movies')\n",
        "plt.ylabel('Director')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW9vfSsf-byu"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code excluding 'Unknown' directors\n",
        "filtered_directors = netflix_data[netflix_data['director'] != 'Unknown']\n",
        "most_filmed_director = filtered_directors['director'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=most_filmed_director.values, y=most_filmed_director.index)\n",
        "plt.title('Top 10 Most Filmed Director (Excluding Unknown)')\n",
        "plt.xlabel('Number of Movies')\n",
        "plt.ylabel('Director')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "Top Filmed Directors Comparison:\n",
        "\n",
        "A horizontal bar chart is chosen to compare the top 10 directors based on the number of movies they have directed in the dataset.\n",
        "\n",
        "Visualizing Director Popularity:\n",
        "\n",
        "The chart provides a clear visualization of the directors with the highest number of films, making it easy to compare their frequencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "Prolific Directors:\n",
        "\n",
        "The chart highlights directors who have directed a substantial number of movies available on the platform.\n",
        "\n",
        "Director Diversity:\n",
        "\n",
        "Users can see a variety of directors in the top 10, showcasing diversity in the contributions of different filmmakers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Knowledge of the most prolific directors can be valuable for content curation and recommendation algorithms. It can also be used in marketing efforts, showcasing movies directed by popular filmmakers to attract viewers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(6,4))\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_data = netflix_data.select_dtypes(include=['float', 'int'])\n",
        "sns.heatmap(numeric_data.corr(),annot=True)\n",
        "plt.show() # Add this line to display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "Understanding Feature Relationships:\n",
        "\n",
        "A correlation heatmap is chosen to visually represent the correlation between different numerical features in the dataset.\n",
        "\n",
        "Identifying Correlations:\n",
        "\n",
        "The heatmap provides a quick overview of which features have positive, negative, or no correlation with each other.\n",
        "\n",
        "Correlation Strength:\n",
        "\n",
        "Colors on the heatmap indicate the strength of correlation, making it easy to identify strong and weak correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "Feature Relationships:\n",
        "\n",
        "Users can quickly identify which features have a strong correlation, helping understand how variables are related.\n",
        "\n",
        "Correlation Strength:\n",
        "\n",
        "The heatmap color intensity helps in gauging the strength of correlation. Darker colors represent stronger correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "Hypothesis testing involves making statements about a population parameter and then using statistical methods to determine if the data provides enough evidence to reject the null hypothesis. Here are three hypothetical statements based on the dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average release years of movies and TV shows on Netflix.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in the average release years of movies and TV shows on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Separate data for movies and TV shows\n",
        "movies_data = netflix_data[netflix_data['type'] == 'Movie']\n",
        "tv_shows_data = netflix_data[netflix_data['type'] == 'TV Show']\n",
        "\n",
        "# Perform T-Test\n",
        "t_stat, p_value = ttest_ind(movies_data['release_year'], tv_shows_data['release_year'], equal_var=False)\n",
        "\n",
        "# Print the p-value\n",
        "print(f\"P-Value for Hypothesis 1: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "The statistical test used for Hypothesis 1 is the Two-Sample T-Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "The Two-Sample T-Test was chosen because we were comparing the means of two independent groups (movies and TV shows) to determine if there is a significant difference in the average release years. This test is appropriate when dealing with numeric data from two distinct groups, and it helps assess whether the observed differences in the means are statistically significant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the average duration between movies and TV shows on Netflix.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant difference in the average duration between movies and TV shows on Netflix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "print(\"Unique Values in 'duration' column:\")\n",
        "print(netflix_data['duration'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40IwA4Si_1or"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Assuming each 'Season' is equivalent to approximately 10 episodes\n",
        "EPISODES_PER_SEASON = 10\n",
        "\n",
        "# Create a new numeric column for duration\n",
        "netflix_data['duration_numeric'] = netflix_data['duration'].replace({' min': '', ' Seasons': ''}, regex=True)\n",
        "\n",
        "# Convert 'Season' values to numeric (approximating 10 episodes per season)\n",
        "netflix_data['duration_numeric'] = pd.to_numeric(netflix_data['duration_numeric'], errors='coerce')\n",
        "netflix_data['duration_numeric'].fillna(netflix_data['duration'].apply(lambda x: EPISODES_PER_SEASON if 'Season' in x else None), inplace=True)\n",
        "\n",
        "# Separate data for movies and TV shows\n",
        "movies_data = netflix_data[netflix_data['type'] == 'Movie']\n",
        "tv_shows_data = netflix_data[netflix_data['type'] == 'TV Show']\n",
        "\n",
        "# Perform T-Test\n",
        "t_stat, p_value = ttest_ind(movies_data['duration_numeric'], tv_shows_data['duration_numeric'], equal_var=False)\n",
        "\n",
        "# Print the p-value\n",
        "print(f\"P-Value for Hypothesis 2: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "We will perform an independent two-sample t-test to compare the average duration between movies and TV shows on Netflix. The t-test will help us determine whether there is a significant difference in the mean duration of these two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "I chose the independent two-sample t-test for Hypothetical Statement 2 because it is appropriate for comparing the means of two independent groups, in this case, movies and TV shows. The t-test helps us assess whether there is a significant difference in the average duration between these two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "Null Hypothesis (H0): There is no significant association between the content rating (TV-MA, R, PG-13, etc.) and the type of content (movies or TV shows) on Netflix.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant association between the content rating and the type of content on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "print(\"Unique Values in 'rating' column:\")\n",
        "print(netflix_data['rating'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbNCzs8RADnx"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(netflix_data['type'], netflix_data['rating'])\n",
        "\n",
        "# Perform the chi-squared test\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print the p-value\n",
        "print(f\"P-Value for Hypothesis 3: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "We perform a chi-squared test of independence. This test is appropriate for examining the association between two categorical variables, in this case, the content rating and the type of content (movies or TV shows)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "I chose the chi-squared test of independence for Hypothetical Statement 3 because it is suitable for examining the association between two categorical variables. In this case, we want to determine if there is a significant association between the content rating (a categorical variable) and the type of content (movies or TV shows, also a categorical variable) on Netflix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvoplbz6AV20"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable it their are more unique than we can drop those columns.\n",
        "netflix_data.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3nTTZFnAYKl"
      },
      "outputs": [],
      "source": [
        "#Dropping show_id, title, director, cast and description for more Unique values\n",
        "netflix_data.drop(['title'], axis=1, inplace=True)\n",
        "netflix_data.drop(['director'], axis=1, inplace=True)\n",
        "netflix_data.drop(['cast'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9_G3AmqAZzh"
      },
      "outputs": [],
      "source": [
        "#assigning customerID to a variable for further use\n",
        "a = netflix_data['show_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UumZlSXSAbp2"
      },
      "outputs": [],
      "source": [
        "#Dropping ID for more Unique values\n",
        "netflix_data.drop(['show_id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRT5BTYEAhmX"
      },
      "outputs": [],
      "source": [
        "# To see the null or missing values\n",
        "netflix_data.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmLqY_atAsN4"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Drop rows with missing values in critical columns or use imputation techniques based on the context\n",
        "netflix_data.dropna(subset=['date_added'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1vFBpZ7AtqK"
      },
      "outputs": [],
      "source": [
        "# You may choose to drop rows with missing ratings or impute based on the mode\n",
        "netflix_data['rating'].fillna(netflix_data['rating'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJOQHTB9Avj4"
      },
      "outputs": [],
      "source": [
        "# To re-check the null or missing values\n",
        "netflix_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Categorical Columns ('country'): Filled missing values with 'Unknown'.\n",
        "\n",
        "Reason: For categorical columns representing country filling missing values with 'Unknown' is a common approach. This placeholder indicates that the information is unknown or not available. It's a simple and interpretable method for handling missing categorical data.\n",
        "\n",
        "Date Column ('date_added'): Dropped rows with missing values.\n",
        "\n",
        "Reason: In the 'date_added' column, the code chose to drop rows with missing date values. Alternatively, you could have chosen to keep these rows or impute the missing dates based on some criterion. The decision depends on the significance of the 'date_added' column for your analysis.\n",
        "\n",
        "Categorical Column ('rating'): Filled missing values with the mode (most frequent value) of the column.\n",
        "\n",
        "Reason: Filling missing values in the 'rating' column with the mode is a common strategy for categorical data. The mode represents the most frequently occurring value, providing a reasonable estimate for the missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Calculate the IQR (Interquartile Range)\n",
        "Q1 = netflix_data['release_year'].quantile(0.25)\n",
        "Q3 = netflix_data['release_year'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print('IQR value is', IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRriA5IVCHZ2"
      },
      "outputs": [],
      "source": [
        "# Define upper and lower bounds to identify outliers\n",
        "lower_bound = Q1 - (1.5 * IQR)\n",
        "upper_bound = Q3 + (1.5 * IQR)\n",
        "print('Lower fence is', lower_bound)\n",
        "print('Upper fence is', upper_bound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqtTnpukA5l3"
      },
      "outputs": [],
      "source": [
        "# Identify and potentially remove outliers\n",
        "outliers = netflix_data[(netflix_data['release_year'] < lower_bound) | (netflix_data['release_year'] > upper_bound)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEZ_gm15A6SU"
      },
      "outputs": [],
      "source": [
        "# Remove outliers from the dataset\n",
        "data = netflix_data[(netflix_data['release_year'] >= lower_bound) & (netflix_data['release_year'] <= upper_bound)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfSbDjq7A7_U"
      },
      "outputs": [],
      "source": [
        "# Boxplot after removies outliers\n",
        "data.boxplot(column =['release_year'], grid = False)\n",
        "plt.title('Outliers removed in release year')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Outlier Treatment Techniques Used:\n",
        "\n",
        "IQR (Interquartile Range) Method:\n",
        "\n",
        "Description: Calculated the Interquartile Range (IQR) for the 'release_year' feature. Reasoning: IQR is a robust method for identifying outliers. It defines a range within which most of the data points lie. Any data points outside this range are considered potential outliers.\n",
        "\n",
        "IQR is a widely accepted method for identifying outliers because it is less sensitive to extreme values compared to other methods. It is suitable for datasets where the distribution is not necessarily normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3-3MFymaLnf"
      },
      "outputs": [],
      "source": [
        "netflix_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "num_cols=[\"release_year\",\"added_day\",\"added_month\",\"added_year\",\"duration_numeric\"]\n",
        "cat_cols=[\"type\",\"country\",\"rating\",\"duration\",\"listed_in\",\"description\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLoWivevaRLC"
      },
      "outputs": [],
      "source": [
        "#changing object to category data type\n",
        "netflix_data[cat_cols] = netflix_data[cat_cols].astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFefWzDGaTCN"
      },
      "outputs": [],
      "source": [
        "netflix_data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "In label encoding, each category is assigned a unique integer label. This encoding is suitable when there is an ordinal relationship among the categories, meaning there is a meaningful order or ranking. However, it's crucial to be cautious, as some machine learning algorithms might misinterpret the ordinal relationships as meaningful numeric distances.\n",
        "\n",
        "Reasons for Using Label Encoding:\n",
        "\n",
        "Suitable for ordinal categorical variables with a clear ranking.\n",
        "\n",
        "Reduces the dimensionality of the data compared to one-hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "contractions_dict = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "# Assuming you have NaN values in 'description', replace them with an empty string\n",
        "netflix_data['description'] = netflix_data['description'].replace(np.nan, '', regex=True)\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for contraction, expansion in contractions_dict.items():\n",
        "        text = text.replace(contraction, expansion)\n",
        "    return text\n",
        "\n",
        "# Apply the function to the 'description' column\n",
        "netflix_data['description_expanded'] = netflix_data['description'].apply(expand_contractions)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description', 'description_expanded']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "# Lowercasing the 'description_expanded' column\n",
        "netflix_data['description_expanded_lower'] = netflix_data['description_expanded'].str.lower()\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_expanded', 'description_expanded_lower']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations\n",
        "import re\n",
        "\n",
        "# Remove punctuations from the 'description_expanded_lower' column\n",
        "netflix_data['description_no_punctuations'] = netflix_data['description_expanded_lower'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_expanded_lower', 'description_no_punctuations']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "# Remove URLs from the 'description_no_punctuations' column\n",
        "netflix_data['description_no_urls'] = netflix_data['description_no_punctuations'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))\n",
        "\n",
        "# Remove words and digits containing digits from the 'description_no_urls' column\n",
        "netflix_data['description_no_digits'] = netflix_data['description_no_urls'].apply(lambda x: re.sub(r'\\b\\w*[0-9]+\\w*\\b', '', x))\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_no_punctuations', 'description_no_urls', 'description_no_digits']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    print(stop_words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the function to the 'description_no_digits' column\n",
        "netflix_data['description_no_stopwords'] = netflix_data['description_no_digits'].apply(remove_stopwords)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_no_digits', 'description_no_stopwords']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces\n",
        "# Define a function to remove white spaces\n",
        "def remove_white_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "# Apply the function to the 'description_no_stopwords' column\n",
        "netflix_data['description_cleaned'] = netflix_data['description_no_stopwords'].apply(remove_white_spaces)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_no_stopwords', 'description_cleaned']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text\n",
        "import random\n",
        "\n",
        "# Function to rephrase text\n",
        "def rephrase_text(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words)):\n",
        "        if random.choice([True, False]):\n",
        "            words[i] = \"synonym_of_\" + words[i]  # Replace with synonym or modify as needed\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the function to the 'description_cleaned' column\n",
        "netflix_data['description_rephrased'] = netflix_data['description_cleaned'].apply(rephrase_text)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_cleaned', 'description_rephrased']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCqfwp9_bXFI"
      },
      "source": [
        "Rephrasing Text:\n",
        "\n",
        "Purpose: Involves rephrasing or rewriting the text.\n",
        "\n",
        "Rephrasing may be used for various reasons, such as improving clarity or expressing the same content differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9GMWBhbgWb"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Apply the function to the 'description_rephrased' column\n",
        "netflix_data['description_tokenized'] = netflix_data['description_rephrased'].apply(tokenize_text)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_rephrased', 'description_tokenized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuZuCmHbkMd"
      },
      "source": [
        "Tokenization:\n",
        "\n",
        "Purpose: Tokenization involves breaking text into individual words or tokens.\n",
        "\n",
        "Tokenization is a fundamental step in natural language processing, enabling the analysis of individual words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ax7B4Hvbnyc"
      },
      "outputs": [],
      "source": [
        "# Download the 'wordnet' resource\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Apply the function to the 'description_tokenized' column\n",
        "netflix_data['description_lemmatized'] = netflix_data['description_tokenized'].apply(lemmatize_tokens)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_tokenized', 'description_lemmatized']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "The text normalization technique used in the provided code is Lemmatization. Specifically, the code utilizes the WordNet Lemmatizer from the Natural Language Toolkit (NLTK) library to lemmatize the tokens in the 'description_tokenized' column.\n",
        "\n",
        "Lemmatization:\n",
        "\n",
        "Purpose: Lemmatization is the process of reducing words to their base or root form (lemma). It involves considering the meaning of a word and transforming it to a common base form.\n",
        "\n",
        "Lemmatization is preferred over stemming in some cases because it produces valid words and retains the base meaning of words. It helps in reducing inflected words to a common form, which can be beneficial for text analysis, information retrieval, and other natural language processing tasks. Lemmatization improves the interpretability of text data and can be particularly useful in tasks where word semantics are crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WYNxN2PbymA"
      },
      "outputs": [],
      "source": [
        "# Download the 'averaged_perceptron_tagger' resource\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Function to perform POS tagging on a list of tokens\n",
        "def pos_tagging(tokens):\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Apply the function to the 'description_tokenized' column\n",
        "netflix_data['description_pos_tags'] = netflix_data['description_tokenized'].apply(pos_tagging)\n",
        "\n",
        "# Display the first few rows\n",
        "netflix_data[['description_tokenized', 'description_pos_tags']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NaRHBJ7b3nI"
      },
      "source": [
        "Part of Speech (POS) Tagging:\n",
        "\n",
        "Purpose: POS tagging assigns a grammatical category (such as noun, verb, adjective) to each word in a text.\n",
        "\n",
        "POS tagging provides information about the grammatical structure of the text, aiding in more advanced linguistic analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'description_tokenized' is a list of tokenized descriptions\n",
        "# Convert tokenized descriptions back to text (assuming they are lists of words)\n",
        "netflix_data['description_tokenized_text'] = netflix_data['description_tokenized'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the 'description_tokenized_text' column\n",
        "description_bow = vectorizer.fit_transform(netflix_data['description_tokenized_text'])\n",
        "\n",
        "# Convert the result to a dense array\n",
        "description_bow_array = description_bow.toarray()\n",
        "\n",
        "# Calculate the average word vector for each description\n",
        "average_word_vector = np.mean(description_bow_array, axis=1)\n",
        "\n",
        "# Create a DataFrame with the average word vectors\n",
        "average_word_vector_df = pd.DataFrame(average_word_vector, columns=['average_word_vector'])\n",
        "\n",
        "# Display the DataFrame\n",
        "average_word_vector_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "The code you provided seems to be for the Bag of Words (BoW) technique, specifically using the CountVectorizer from scikit-learn. BoW is a simple and effective text vectorization technique that represents text data as a sparse matrix of word frequencies.\n",
        "\n",
        "Simplicity and Effectiveness: BoW is a straightforward and efficient method for converting text into numerical features. It disregards the order of words and focuses on their frequency, making it suitable for various natural language processing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "netflix_data.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZngHqDDcIwS"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(6,4))\n",
        "# sns.heatmap(netflix_data.corr(),annot=True)\n",
        "# Drop non-numerical columns before calculating correlation\n",
        "numerical_data = netflix_data.select_dtypes(include=['number'])\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(numerical_data.corr(),annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# List of columns to drop\n",
        "columns_to_drop = [ 'description',\n",
        "    'description_expanded',\n",
        "    'description_expanded_lower',\n",
        "    'description_no_punctuations',\n",
        "    'description_no_urls',\n",
        "    'description_no_digits',\n",
        "    'description_no_stopwords',\n",
        "    'description_cleaned',\n",
        "    'description_rephrased',\n",
        "    'description_tokenized',\n",
        "    'description_lemmatized',\n",
        "    'description_pos_tags'\n",
        "]\n",
        "# Drop the specified columns\n",
        "netflix_data = netflix_data.drop(columns=columns_to_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubLEFribcccN"
      },
      "outputs": [],
      "source": [
        "# Display the updated DataFrame\n",
        "netflix_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d37rUgQScqfI"
      },
      "source": [
        "No need of Data Transformation know because we have already done all the things which I am writing below:\n",
        "\n",
        "Data Cleaning:\n",
        "\n",
        "Handling missing values in columns like date_added, duration, etc. Dropping unnecessary columns like show_id and description based on your requirements.\n",
        "\n",
        "Data Wrangling:\n",
        "\n",
        "Converting the date_added column to datetime format. Creating a new column duration_numeric based on the numeric representation of the duration.\n",
        "\n",
        "Statistical Analysis:\n",
        "\n",
        "Conducting hypothesis testing to derive insights from the dataset.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Creating new features like added_day, added_month, and added_year from the date_added column. Converting categorical columns to category data type for better representation.\n",
        "\n",
        "Handling Outliers:\n",
        "\n",
        "Identifying and handling outliers in the release_year and duration columns.\n",
        "\n",
        "Data Normalization:\n",
        "\n",
        "Scaling numerical features using StandardScaler.\n",
        "\n",
        "Textual Data Preprocessing:\n",
        "\n",
        "Tokenization and lemmatization of the description column.\n",
        "\n",
        "Feature Manipulation:\n",
        "\n",
        "Standardizing numerical features using StandardScaler. Encoding categorical columns, such as type, country, rating, duration, listed_in, and description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Splliting up of data\n",
        "X = netflix_data.drop([\"type\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ8nfmNEczG2"
      },
      "outputs": [],
      "source": [
        "y = netflix_data[[\"type\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FLY1cm5c1UZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqOEMxoOc3Is"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FacPE59pc5Cu"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[num_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUiM1q7Pc8tK"
      },
      "outputs": [],
      "source": [
        "X_train_std = scaler.transform(X_train[num_cols])\n",
        "X_test_std = scaler.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkYHnGnxc-hu"
      },
      "outputs": [],
      "source": [
        "print(X_train_std.shape)\n",
        "print(X_test_std.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "The StandardScaler from scikit-learn to scale the numerical columns in your dataset. The StandardScaler standardizes features by removing the mean and scaling to unit variance. This method is commonly used because it assumes that the distribution of the data is Gaussian, which is often a good assumption. Scaling is essential for many machine learning algorithms, especially those that are sensitive to the scale of input features, such as distance-based algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "No need\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihftMT0EdZ3t"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the training and testing sets\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goHGuePcdblw"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R28EacVoddut"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts(normalize=True)*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6a_tSyTdf7f"
      },
      "outputs": [],
      "source": [
        "y_test.value_counts(normalize=True)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "The data has been split into training and testing sets with a ratio of approximately 80% for training and 20% for testing. This is a common and reasonable split, and the specific ratio depends on factors like the size of your dataset, the complexity of your model, and your specific use case.\n",
        "\n",
        "A common practice is the 80-20 split, where 80% of the data is used for training to ensure the model learns patterns in the majority of the data, and 20% is reserved for testing to evaluate how well the model generalizes to new, unseen data. However, other ratios like 70-30 or 75-25 are also used depending on the scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "In this context, if the 'type' column has only two categorical values ('TV Show' and 'Movie'), and the distribution between these two categories is significantly skewed, then the dataset is indeed imbalanced. Imbalanced datasets can sometimes lead to biased model training, as the model might become more proficient at predicting the majority class and less effective at predicting the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DkAE_Njdr7L"
      },
      "outputs": [],
      "source": [
        "y_train['type_enc'] = LabelEncoder().fit_transform(y_train['type'])\n",
        "y_train[['type', 'type_enc']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um9-1Z5NdtxT"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.drop(columns=[\"type\"])\n",
        "y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIXnqX59dvqi"
      },
      "outputs": [],
      "source": [
        "y_test['type_enc'] = LabelEncoder().fit_transform(y_test['type'])\n",
        "y_test[['type', 'type_enc']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWJaW-Kadx2h"
      },
      "outputs": [],
      "source": [
        "y_test = y_test.drop(columns=[\"type\"])\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nZBbzMPdzwy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpJ3fWkud1qk"
      },
      "outputs": [],
      "source": [
        "cat_cols = X_train.select_dtypes(include=['category']).columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PadfpYKVd4q3"
      },
      "outputs": [],
      "source": [
        "# Initialize the OneHotEncoder\n",
        "enc = OneHotEncoder(drop = 'first')\n",
        "enc.fit(X_train[cat_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qangof45d7Ci"
      },
      "outputs": [],
      "source": [
        "# Initialize the OneHotEncoder with handle_unknown='ignore'\n",
        "enc = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
        "\n",
        "# Perform one-hot encoding on the 'type' column for training and testing sets\n",
        "X_train_ohe = enc.fit_transform(X_train[cat_cols]).toarray()\n",
        "X_test_ohe = enc.transform(X_test[cat_cols]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk8suV0ed8xn"
      },
      "outputs": [],
      "source": [
        "print(X_train_ohe.shape)\n",
        "print(X_test_ohe.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "I used one-hot encoding to handle the imbalance in the 'type' column. This is a valid approach as it creates binary columns for each category in the target variable, effectively turning it into a binary classification problem. The drop_first=True parameter in pd.get_dummies() removes one of the binary columns to avoid multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl-2FHLReCGk"
      },
      "outputs": [],
      "source": [
        "X_train_con = np.concatenate([X_train_std, X_train_ohe], axis=1)\n",
        "X_test_con = np.concatenate([X_test_std, X_test_ohe], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMBWK-mGeDq5"
      },
      "outputs": [],
      "source": [
        "print(X_train_con.shape)\n",
        "print(X_test_con.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlX8b-EweJNQ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(act, pred):\n",
        "    from sklearn.metrics import confusion_matrix,classification_report, accuracy_score, recall_score, precision_score, f1_score\n",
        "    print(\"Confusion Matrix \\n\", confusion_matrix(act, pred))\n",
        "    print(classification_report(act,pred))\n",
        "    print(\"Accurcay : \", accuracy_score(act, pred))\n",
        "    print(\"Recall   : \", recall_score(act, pred,average='weighted'))\n",
        "    print(\"Precision: \", precision_score(act, pred, average='weighted'))\n",
        "    print(\"F1_score : \", f1_score(act, pred, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVAH2b5xeLPH"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=123)\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train_con, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xJf6Y4CeNeu"
      },
      "outputs": [],
      "source": [
        "np.unique(y_train, return_counts= True)\n",
        "np.unique(y_train_sm, return_counts= True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "m1 = LogisticRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "m1.fit(X_train_con, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_lr = m1.predict(X_train_con)\n",
        "test_pred_lr = m1.predict(X_test_con)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train, train_pred_lr)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "m2 = LogisticRegression(solver='saga',penalty='l2', max_iter=1000)\n",
        "\n",
        "# Fit the Algorithm\n",
        "m2.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_lr_hp = m2.predict(X_train_con)\n",
        "test_pred_lr_hp = m2.predict(X_test_con)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBAozuUhegOL"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train, train_pred_lr_hp)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_lr_hp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "solver='saga': Specifies the algorithm to use in the optimization problem. The 'saga' solver is often suitable for large datasets.\n",
        "\n",
        "penalty='l2': Indicates the type of regularization term to be applied. 'l2' refers to the Ridge regularization.\n",
        "\n",
        "max_iter=1000: Defines the maximum number of iterations taken for the solver to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Comparison:\n",
        "\n",
        "Accuracy: The accuracy is slightly lower after hyperparameter tuning, but it remains high, indicating that the model generalizes well to the test set.\n",
        "\n",
        "Recall: Recall values are still close to 1, suggesting that the models effectively identify positive instances.\n",
        "\n",
        "Precision: Precision values are also close to 1, indicating a low rate of false positives.\n",
        "\n",
        "F1-score: F1-scores are high, reflecting a good balance between precision and recall.\n",
        "\n",
        "In summary, while there is a slight decrease in accuracy after hyperparameter tuning, the model still performs exceptionally well on both the training and testing sets. The differences are subtle, and the impact on overall model performance seems minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqTCURw8eyiR"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "m3 = RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "m3.fit(X_train_con, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_rf = m3.predict(X_train_con)\n",
        "test_pred_rf = m3.predict(X_test_con)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train, train_pred_rf)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVI1o1zue8fS"
      },
      "outputs": [],
      "source": [
        "param_grid = {\"n_estimators\" : [100,300,700],\n",
        "              \"max_depth\" : [3,5,7,11],\n",
        "              \"max_features\" : [3,5,7,9],\n",
        "              \"min_samples_leaf\" : [2,4,6]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MAPHd-De-Vr"
      },
      "outputs": [],
      "source": [
        "#Returning the best combination of parameters\n",
        "#specifing the no.of folds\n",
        "#m4 = RandomForestClassifier()\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#m4 = GridSearchCV(m4,param_grid,cv=5)\n",
        "#m4.fit(X_train_con,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiLWDAnSfBXx"
      },
      "outputs": [],
      "source": [
        "#m4.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "m4 = RandomForestClassifier(max_depth=11,n_estimators=100,max_features=9,min_samples_leaf=2)\n",
        "\n",
        "# Fit the Algorithm\n",
        "m4.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_rf_hp = m4.predict(X_train_sm)\n",
        "test_pred_rf_hp = m4.predict(X_test_con)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syPngwqvfIRe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train_sm, train_pred_rf_hp)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_rf_hp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "max_depth: Maximum depth of the trees\n",
        "\n",
        "n_estimators: Number of trees in the forest\n",
        "\n",
        "max_features: Maximum number of features considered for splitting a node\n",
        "\n",
        "min_samples_leaf: Minimum number of samples required to be at a leaf node\n",
        "\n",
        "The hyperparameter optimization technique used in this case is manual tuning, where you've manually selected values for the hyperparameters based on your understanding of the model and the dataset. This approach is reasonable, especially when you have some prior knowledge about the hyperparameters and their effects on the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "There is a slight decrease in both train and test accuracy after hyperparameter tuning. While the model's performance on the training set is still excellent, there might be a concern about potential overfitting as the model's performance on the test set has slightly decreased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Accuracy:\n",
        "\n",
        "Indication: It measures the overall correctness of the model predictions.\n",
        "\n",
        "Business Impact: High accuracy indicates that the model is making correct predictions, which is generally desirable. However, in imbalanced datasets, accuracy alone may not provide a complete picture, especially if one class dominates the dataset. For example, if a model predicts the majority class all the time, it could still achieve high accuracy but may not be useful.\n",
        "\n",
        "F1-Score:\n",
        "\n",
        "Indication: F1-Score is the weighted average of precision and recall. It considers both false positives and false negatives.\n",
        "\n",
        "Business Impact: F1-Score is beneficial when there is an uneven class distribution. It balances precision and recall and is useful in scenarios where both false positives and false negatives are equally important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn import svm\n",
        "m5 = svm.SVC(kernel='linear')\n",
        "\n",
        "# Fit the Algorithm\n",
        "m5 = m5.fit(X_train_con,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_svm = m5.predict(X_train_con)\n",
        "test_pred_svm = m5.predict(X_test_con)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train, train_pred_svm)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_svm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "m6 = svm.SVC(kernel='rbf',C=10)\n",
        "\n",
        "# Fit the Algorithm\n",
        "m6.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "# Predict on the model\n",
        "train_pred_svm1 = m6.predict(X_train_sm)\n",
        "test_pred_svm1 = m6.predict(X_test_con)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8EymMJUfvNz"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "print(\"--Train--\")\n",
        "evaluate_model(y_train_sm, train_pred_svm1)\n",
        "print(\"--Test--\")\n",
        "evaluate_model(y_test, test_pred_svm1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "In your implementation, you have used two different kernels for Support Vector Machines (SVM) and adjusted hyperparameters manually.\n",
        "\n",
        "Implementation: svm.SVC(kernel='linear')\n",
        "\n",
        "The linear kernel, which is suitable for linearly separable data. The model performed exceptionally well, achieving high accuracy on both the training and test sets.\n",
        "\n",
        "Implementation: svm.SVC(kernel='rbf', C=10)\n",
        "\n",
        "The radial basis function (RBF) kernel and manually set the regularization parameter C to 10. The model achieved high accuracy on both training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "Comparison:\n",
        "\n",
        "The accuracy on the test set slightly decreased after hyperparameter tuning (from 99.87% to 99.81%).\n",
        "\n",
        "However, the differences are minimal, and both models exhibit excellent performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixmrw8OAf8Z0"
      },
      "outputs": [],
      "source": [
        "performance_columns = ['Model name', 'Train accuracy', 'Train precision', 'Train recall','Train F1_score',\n",
        "                       'Test accuracy', 'Test precision', 'Test recall','Test F1_score']\n",
        "performance_comparison = pd.DataFrame(columns=performance_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTkpoZiYgCiL"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import average\n",
        "def add_to_perform_compare_df(df, model_name, train_actual, train_predict, test_actual, test_predict):\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "    train_accuracy = accuracy_score(train_actual, train_predict)\n",
        "    test_accuracy = accuracy_score(test_actual, test_predict)\n",
        "\n",
        "    train_recall = recall_score(train_actual, train_predict,average='weighted')\n",
        "    test_recall = recall_score(test_actual,test_predict,average='weighted')\n",
        "\n",
        "    train_precision = precision_score(train_actual, train_predict,average='weighted')\n",
        "    test_precision = precision_score(test_actual, test_predict,average='weighted')\n",
        "\n",
        "    train_f1 = f1_score(train_actual, train_predict,average='weighted')\n",
        "    test_f1 = f1_score(test_actual, test_predict,average='weighted')\n",
        "\n",
        "    df = df.append(pd.Series([model_name, train_accuracy, train_precision, train_recall, train_f1,\n",
        "                              test_accuracy, test_precision, test_recall,test_f1],\n",
        "                             index=df.columns),ignore_index=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z74o4IXogFT6"
      },
      "source": [
        "Precision, Recall, and F1-Score:\n",
        "\n",
        "Consideration: These metrics are valuable in scenarios where class distribution is imbalanced. Precision focuses on the accuracy of positive predictions, recall on the model's ability to find all positive instances, and the F1-score balances precision and recall. Use Case: Especially relevant when the cost of false positives or false negatives is different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vGZ_tMwgY4p"
      },
      "outputs": [],
      "source": [
        "netflix_data.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLO3U7JGgbWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains your target variable\n",
        "X = netflix_data.drop('type', axis=1)\n",
        "y = netflix_data['type']\n",
        "\n",
        "# Exclude datetime column\n",
        "X_numeric = X.select_dtypes(include=['number'])\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "X_encoded = pd.get_dummies(X_numeric, drop_first=True)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.cluster import KMeans # Import KMeans\n",
        "\n",
        "# Assuming 'X' contains your feature columns and 'y' contains your target variable\n",
        "X = netflix_data.drop('type', axis=1)\n",
        "y = netflix_data['type']\n",
        "\n",
        "# Exclude datetime column\n",
        "X_numeric = X.select_dtypes(include=['number'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4urJEJmgrX8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Concatenate scaled numeric features and one-hot encoded categorical features for training and testing sets\n",
        "X_train_con = np.concatenate([X_train_std, X_train_ohe], axis=1)\n",
        "X_test_con = np.concatenate([X_test_std, X_test_ohe], axis=1)\n",
        "\n",
        "# Choose the number of clusters (you need to decide the optimal number based on your data or use techniques like elbow method)\n",
        "num_clusters = 3  # You can change this number based on your analysis\n",
        "\n",
        "# Fit KMeans on the training data\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X_train_con)\n",
        "\n",
        "# Predict clusters for training and testing data\n",
        "train_clusters = kmeans.predict(X_train_con)\n",
        "test_clusters = kmeans.predict(X_test_con)\n",
        "\n",
        "# Add the predicted clusters to your original dataframes if needed\n",
        "X_train['cluster'] = train_clusters\n",
        "X_test['cluster'] = test_clusters\n",
        "\n",
        "# Print the clusters for training and testing sets\n",
        "print(\"Training Set Clusters:\")\n",
        "print(X_train['cluster'].value_counts())\n",
        "\n",
        "print(\"\\nTesting Set Clusters:\")\n",
        "print(X_test['cluster'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqsg5ofuguy9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use PCA to reduce the dimensionality for visualization (you can adjust the number of components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_con)\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Scatter plot for training set\n",
        "for cluster in range(num_clusters):\n",
        "    plt.scatter(X_train_pca[train_clusters == cluster, 0],\n",
        "                X_train_pca[train_clusters == cluster, 1],\n",
        "                label=f'Cluster {cluster + 1}')\n",
        "\n",
        "plt.title('KMeans Clustering - Training Set')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY7hQDl1guvi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Assuming 'metrics_data' is a DataFrame with your model metrics\n",
        "metrics_data = pd.DataFrame({\n",
        "    'Model name': ['Logistic Regression', 'Logistic Regression_HP', 'Random Forest', 'Random Forest_HP', 'SVM tune1'],\n",
        "    'Train accuracy': [0.998393, 0.997910, 1.000000, 0.994067, 1.000000],\n",
        "    'Train precision': [0.998401, 0.997924, 1.000000, 0.994084, 1.000000],\n",
        "    'Train recall': [0.998393, 0.997910, 1.000000, 0.994067, 1.000000],\n",
        "    'Train F1_score': [0.998394, 0.997912, 1.000000, 0.994067, 1.000000],\n",
        "    'Test accuracy': [0.996787, 0.996144, 0.998715, 0.989075, 0.998072],\n",
        "    'Test precision': [0.996820, 0.996192, 0.998715, 0.989120, 0.998084],\n",
        "    'Test recall': [0.996787, 0.996144, 0.998715, 0.989075, 0.998072],\n",
        "    'Test F1_score': [0.996791, 0.996151, 0.998715, 0.989039, 0.998074]\n",
        "})\n",
        "\n",
        "# Specify the filename for the CSV file\n",
        "csv_filename = 'model_metrics.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "metrics_data.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f'The model metrics have been saved to {csv_filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Data Overview:\n",
        "\n",
        "The dataset provides information on Netflix content, including movies and TV shows, with details such as release year, country, rating, and more.\n",
        "\n",
        "Data Exploration and Visualization:\n",
        "\n",
        "Explored the distribution of content types (movies vs. TV shows), release years, and content ratings. Investigated the top countries contributing to Netflix content and identified popular genres and directors.\n",
        "\n",
        "Insights from Visualizations:\n",
        "\n",
        "The majority of content on Netflix is movies. Content has been steadily increasing over the years, with a surge in recent years. Certain countries, genres, and directors dominate the Netflix platform.\n",
        "\n",
        "Machine Learning Models:\n",
        "\n",
        "Implemented machine learning models, including Logistic Regression and Random Forest, to predict certain aspects of the dataset. Conducted hyperparameter optimization to enhance model performance.\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Assessed model performance using various metrics such as accuracy, precision, recall, and F1-score. Logistic Regression, Random Forest, and SVM demonstrated high accuracy and generalization to test data.\n",
        "\n",
        "Feature Importance:\n",
        "\n",
        "Explored feature importance using Random Forest, identifying key factors influencing predictions.\n",
        "\n",
        "Business Implications:\n",
        "\n",
        "Insights into content popularity, user preferences, and factors affecting viewership can guide content creation and acquisition strategies. Machine learning models can assist in recommending content and optimizing user engagement.\n",
        "\n",
        "Limitations and Further Work:\n",
        "\n",
        "Considered limitations, such as potential biases in the dataset or the need for more granular data. Proposed areas for further analysis or improvements in predictive models.\n",
        "\n",
        "In conclusion, the analysis provides valuable insights into Netflix content trends and user engagement. The machine learning models exhibit strong predictive capabilities, offering potential applications for content recommendation and business strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "mDgbUHAGgjLW",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "yiiVWRdJDDil",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
